{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:59:44.384038Z",
     "start_time": "2020-07-19T09:59:41.683236Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:01:32.179537Z",
     "start_time": "2020-07-19T10:01:32.159240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'i': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'cat': 5,\n",
       " 'do': 6,\n",
       " 'you': 7,\n",
       " 'think': 8,\n",
       " 'amazing': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'I love my dog!',\n",
    "    'Do you think my dog amazing?'\n",
    "]\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:01:33.668281Z",
     "start_time": "2020-07-19T10:01:33.659501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1, 4], [2, 3, 1, 5], [2, 3, 1, 4], [6, 7, 8, 1, 4, 9]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling unknown words\n",
    "In order to handle texts or words not seen in the training data for the neural network, we can use 'out of vocabulory', so that it is tokenized and the lenght of the sentense is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:05:04.917830Z",
     "start_time": "2020-07-19T10:05:04.909299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE USING OUT OF VOCABULORY: \n",
      "[[2, 1, 4], [1, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE USING OUT OF VOCABULORY: \")\n",
    "test_data = [\n",
    "    'I really like my dog',\n",
    "    'My dog name is rocky'\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:06:23.254765Z",
     "start_time": "2020-07-19T10:06:23.241925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'i': 3, 'love': 4, 'dog': 5, 'cat': 6, 'do': 7, 'you': 8, 'think': 9, 'amazing': 10}\n",
      "\n",
      "BEFORE USING OUT OF VOCABULORY: \n",
      "\n",
      "\n",
      "[[3, 1, 1, 2, 5], [2, 5, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "print()\n",
    "print(\"BEFORE USING OUT OF VOCABULORY: \")\n",
    "print()\n",
    "test_data = [\n",
    "    'I really like my dog',\n",
    "    'My dog name is rocky'\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print()\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling sentences of different lengths\n",
    "* In neural networks all the input data needs to be of same length\n",
    "* Hence we are using padding here\n",
    "* Padding can be at the start in sequence or end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:09:59.034717Z",
     "start_time": "2020-07-19T10:09:59.023793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 3, 1, 4],\n",
       "       [0, 0, 2, 3, 1, 5],\n",
       "       [0, 0, 2, 3, 1, 4],\n",
       "       [6, 7, 8, 1, 4, 9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:10:31.100386Z",
     "start_time": "2020-07-19T10:10:31.086679Z"
    }
   },
   "source": [
    "* padding example after the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:10:53.411338Z",
     "start_time": "2020-07-19T10:10:53.393800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 1, 4, 0, 0],\n",
       "       [2, 3, 1, 5, 0, 0],\n",
       "       [2, 3, 1, 4, 0, 0],\n",
       "       [6, 7, 8, 1, 4, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:11:25.929547Z",
     "start_time": "2020-07-19T10:11:25.920428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 1, 4, 0],\n",
       "       [2, 3, 1, 5, 0],\n",
       "       [2, 3, 1, 4, 0],\n",
       "       [6, 7, 8, 1, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5, truncating='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
